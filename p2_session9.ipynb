{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "p2_session9.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMUpyGN8HKR/duGa0P/fLN7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkhetan/ph2_session9/blob/master/p2_session9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWek-Visn0QO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "59cc8610-831d-46df-f6ae-c604c1fde030"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pybullet envs\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.fuctional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-5783a947008d>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    import pybullet envs\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4PD22QuovbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "  def __init__(self, maxsize = 1e6):\n",
        "    self.storage[]\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "\n",
        "def add(self, transition):\n",
        "  if len(self.storage) == self.max_size:\n",
        "    self.storage[int(self.ptr)] = tranistion\n",
        "    self.ptr = (self.ptr + 1) % self.max_size\n",
        "  else:\n",
        "    self.storage.append(transition)\n",
        "\n",
        "def sample(self, batch_size):\n",
        "  ind = np.random.randint(0, len(self.storage), batch_size)\n",
        "  batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [],[],[],[],[]\n",
        "  for i in ind:\n",
        "    state, next_state, action, reward, done = self.storage[i]\n",
        "    batch_states.append(np.array(state, copy = False))\n",
        "    batch_next_states.append(np.array(next_state, copy = False))\n",
        "    batch_actions.append(np.array(action, copy = False))\n",
        "    batch_rewards.append(np.array(reward, copy = False))\n",
        "    batch_dones.append(np.array(done, copy = False))\n",
        "  return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1,-1), np.array(batch_dones).reshape(-1,-1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-m183tgPCbC",
        "colab_type": "text"
      },
      "source": [
        "Actor network - predicts \"actions\" which are nothing but Q values and the action we take is the one that has the max Q value... but let us say - Actor gives us \"actions\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrQjZjpfxN-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Actor(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dims, action_dim, max_action):\n",
        "    #max_action is to clip in case we added too much noise\n",
        "    super(Actor, self).__init__() # activate the inheritence\n",
        "    self.layer_1 = nn.Linear(state_dims, 400)\n",
        "    self.layer_2 = nn.Linear(400,300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.action_max = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action*torch.tanh(self.layer_3(x))\n",
        "    return x\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5nOjx1WPZmI",
        "colab_type": "text"
      },
      "source": [
        "Critic network. There is same definition for critic1 and critic2\n",
        "There is no need to clip actions. This network receives clipped actions only from Actor\n",
        "state_dims - input dimensions... e.g. cartpole input was a patch of image. here it could be many moving parts of a robot for example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Alyl_U1APd0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dims, action_dim):\n",
        "    super(Critic, self).__init__() # activate the inheritence\n",
        "    # First critic network\n",
        "    self.layer_1 = nn.Linear(state_dims + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    # Second Critic network - we are defining it here seperately because it is going to have different\n",
        "    # waights compared to the first critic\n",
        "    self.layer_4 = nn.Linear(state_dims + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, action_dim)\n",
        "\n",
        "  def forward(self, x, u): # x - state, u - action\n",
        "    xu = torch.cat([x, u], 1) # 1 for vertical concat and 0 for horizontal\n",
        "    # forward propagation on first critic\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "\n",
        "    # forward propagation on second Critic\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "\n",
        "    return x1, x2\n",
        "\n",
        "# The following is defined separately because we want to train our actor later on\n",
        "# There will be no back prop on this. That is why it is kept separately\n",
        "  def Q1(self, x, u): # x - state, u = action This is used for updating the Q values\n",
        "    xu = torch.cat([x, u], 1) # 1 for vertical concat and 0 for horizontal\n",
        "    # We decided to use the first critic to train our Actor\n",
        "    # Any of them could have been used - or even avg of 1st and 2nd... but we\n",
        "    # are using the 1st\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKq4O59tl_Ce",
        "colab_type": "text"
      },
      "source": [
        "Training Process - Steps 4 to 15"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuD_nM0EmC_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Selecting the device CPU or GPU\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#Building the whole training processs into a class\n",
        "\n",
        "class T3D(object):\n",
        "  def __init__(self, state_dims, action_dim, max_action):\n",
        "\n",
        "    #making sure our T3D class can work with any env\n",
        "    self.actor = Actor(state_dims, action_dim, max_action).to(device) #GD This is actor model where we are going to run back prop\n",
        "    self.actor_target = Actor(state_dims, action_dim, max_action).to(device) # Polyak avg\n",
        "    self.actor_target.load_state_dict(self, self.actor.state_dict) # during init - the wigths of actor model and actor target are same\n",
        "    # initializing with model weights to keep them same\n",
        "    # Adam optimizer - reinforcement learning uses this instead of SGD\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "\n",
        "    self.critic = Critic(state_dims, action_dim).to(device) # GD\n",
        "    self.critic_target = Critic(state_dims, action_dim).to(device) # Polyak Avg\n",
        "    self.critic_target.load_state_dict(self, critic.state_dict) # during init - the wigths of critic model and critic target are same\n",
        "\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  # we need actions to train our critic\n",
        "  def select_action(self, state):\n",
        "\n",
        "    state = torch.Tensor(state.reshape(1,-1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "    # need to convert to numpy, remeber clipping?\n",
        "\n",
        "  # iterations - how many iterations you want to run the training loop for, discount is gamma, tau is for polyak avg, policy noise is noise we add to our actions\n",
        "  # noise_clip - this is our max action amplitude for our env (moving only in max -.5 quanta)\n",
        "  # policy_freq - how often are we going to update our actor \n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount = 0.99, tau = 0.005, policy_noise = 0.2, noise_clip = 0.5, policy_freq = 2):\n",
        "    for it in range(iterations):\n",
        "      # step 4 is taking a sample from a batch of transitions (s,s',a,r) from memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      # Each of the above need to be sent to our GPU / CPU\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "\n",
        "      # Step 5 : from the next state s', the actor target plays the next action a'\n",
        "      # mayank - doubt - but this is a list of actions - not \"the\" action - isn't it?\n",
        "      next_action = self.actor_target.forward(next_state)\n",
        "\n",
        "      #step 6 : add gausian noise to a' and clamp it to a value that is allowed by our env\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      # So our next_action is next_action from the network + noise - then clamp it by\n",
        "      # limitations of the environment (self.max_action)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "      # step 7 : this next action a' and next state s' is fed to the critics\n",
        "      # Qt1(s',a') and Qt2(s',a')\n",
        "      target_Q1, target_Q2 = self.critic_target.forward(next_state, next_action)\n",
        "\n",
        "      # step 8: take the min of the 2\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "\n",
        "      # Step 9.. so the actual target_Q = reward + gamma*target_Q <--- this target_Q was obtained above\n",
        "      # but we should not consider the steps where done - 1 \n",
        "      target_Q = reward + ((1 - done)*gamma*target_Q).detach # this detach is needed for a reason i do not understand\n",
        "\n",
        "      # step 10: The 2 critic \"models\" take (s,a) and give out Q1 and Q2\n",
        "      current_Q1, current_Q2 = self.critic.forward(self, action)\n",
        "\n",
        "      # step 11: We calculate the loss from the 2 critic models\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "\n",
        "      # step 12: Back prop on critic models using the above loss\n",
        "      self.critic_optimizer.zero_grad() # initializing the gradients to 0\n",
        "      critic_loss.backward() # computing the gradient\n",
        "      self.critic_optimizer.step() # performing the weight updates\n",
        "\n",
        "      # step 13:\n",
        "      # Once every 2 iterations, we update the Actor model by performing gradient ascent\n",
        "      # on the o/p of the \"first\" critic model (we chose it - it could have been 2nd or avg of both)\n",
        "\n",
        "      if it % policy_freq == 0:\n",
        "        # This is DPG part?\n",
        "        # Basically - we run forward propagation on critic model 1 (Q1) - feed it the current state and\n",
        "        # output of actor when the same state was passed... calculate mean of the critic outputs\n",
        "        # But what is this loss compared to? it should be compared to how good this could have been? How \n",
        "        # will we know that?\n",
        "        actor_loss = -(self.critic.Q1(state, self.actor(state)).mean())\n",
        "        self.actor_optimizer.grad_zero()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "      # Step 14: Every 2 iterations, perform polyak averaging on actor target from actor model\n",
        "      # remember - tau is .005 - so, it is taking a very small weight of actor model\n",
        "      for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "        target_param.data.copy_(tau * param.data + (1-tau)*target_param.data)\n",
        "\n",
        "      # step 15: Once every 2 iterations, we update the critic target the same way\n",
        "      for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "        target_param.data.copy_(tau * param.data + (1-tau)*target_param.data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTKdKy5dre2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}